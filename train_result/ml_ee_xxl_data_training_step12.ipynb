{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 321,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2880,
     "status": "error",
     "timestamp": 1505781339378,
     "user": {
      "displayName": "Sara Robinson",
      "photoUrl": "//lh4.googleusercontent.com/-RR9n0dvbwgI/AAAAAAAAAAI/AAAAAAAAMYM/SOr5ZExpvXE/s50-c-k-no/photo.jpg",
      "userId": "112510032804989247452"
     },
     "user_tz": 240
    },
    "id": "783h64rGhA3T",
    "outputId": "d447b2ab-e321-4ee5-abd4-de2c0116302f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\n",
    "\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- We are using CPU now ----\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print('---- We are using GPU now ----')\n",
    "    # GPU\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 80  # Number of passes through entire dataset\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    print('---- We are using CPU now ----')\n",
    "    # CPU\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The wine reviews dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X :(412038, 240, 1)\n",
      "Size of Y :(412038,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/floyd/input/gengduoshuju/'  # ADD path/to/dataset\n",
    "Y= pickle.load( open(os.path.join(data_path,'Y.pks'), \"rb\" ) )\n",
    "X= pickle.load( open(os.path.join(data_path,'X.pks'), \"rb\" ) )\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "print(\"Size of X :\" + str(X.shape))\n",
    "print(\"Size of Y :\" + str(Y.shape))\n",
    "X = X.astype(np.float64)\n",
    "X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "331.0\n",
      "number of training examples = 403797\n",
      "number of test examples = 8241\n",
      "X_train shape: (403797, 240, 1)\n",
      "Y_train shape: (403797, 332)\n",
      "X_test shape: (8241, 240, 1)\n",
      "Y_test shape: (8241, 332)\n"
     ]
    }
   ],
   "source": [
    "X_train,  X_test, Y_train_orig,Y_test_orig= helper.divide_data(X,Y)\n",
    "print(Y.min())\n",
    "print(Y.max())\n",
    "num_classes = 332\n",
    "Y_train = keras.utils.to_categorical(Y_train_orig, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test_orig, num_classes)\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# Load the model what has already ben trained\n",
    "# ===================================================================================\n",
    "\n",
    "model = load_model(r\"floyd_model_xxl_data_ver13.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 240, 16)           80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 240, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 60, 64)            8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 64)            16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 15, 32)            8224      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 332)               85324     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 332)               0         \n",
      "=================================================================\n",
      "Total params: 120,412\n",
      "Trainable params: 120,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 403797 samples, validate on 8241 samples\n",
      "Epoch 1/100\n",
      "403797/403797 [==============================] - 87s 215us/step - loss: 0.0709 - acc: 0.9722 - val_loss: 0.0931 - val_acc: 0.9642\n",
      "Epoch 2/100\n",
      "403797/403797 [==============================] - 83s 207us/step - loss: 0.0721 - acc: 0.9718 - val_loss: 0.0957 - val_acc: 0.9659\n",
      "Epoch 3/100\n",
      "403797/403797 [==============================] - 83s 206us/step - loss: 0.0714 - acc: 0.9722 - val_loss: 0.0934 - val_acc: 0.9680\n",
      "Epoch 4/100\n",
      "403797/403797 [==============================] - 84s 208us/step - loss: 0.0702 - acc: 0.9725 - val_loss: 0.1130 - val_acc: 0.9578\n",
      "Epoch 5/100\n",
      "403797/403797 [==============================] - 84s 208us/step - loss: 0.0707 - acc: 0.9724 - val_loss: 0.0948 - val_acc: 0.9661\n",
      "Epoch 6/100\n",
      "403797/403797 [==============================] - 84s 208us/step - loss: 0.0707 - acc: 0.9723 - val_loss: 0.1011 - val_acc: 0.9635\n",
      "Epoch 7/100\n",
      "403797/403797 [==============================] - 84s 207us/step - loss: 0.0723 - acc: 0.9713 - val_loss: 0.0860 - val_acc: 0.9682\n",
      "Epoch 8/100\n",
      "403797/403797 [==============================] - 84s 207us/step - loss: 0.0717 - acc: 0.9721 - val_loss: 0.0930 - val_acc: 0.9677\n",
      "Epoch 9/100\n",
      "403797/403797 [==============================] - 84s 208us/step - loss: 0.0703 - acc: 0.9721 - val_loss: 0.1034 - val_acc: 0.9623\n",
      "Epoch 10/100\n",
      "403797/403797 [==============================] - 84s 208us/step - loss: 0.0689 - acc: 0.9729 - val_loss: 0.0880 - val_acc: 0.9671\n",
      "Epoch 11/100\n",
      "403797/403797 [==============================] - 83s 206us/step - loss: 0.0705 - acc: 0.9723 - val_loss: 0.1047 - val_acc: 0.9638\n",
      "Epoch 12/100\n",
      "403797/403797 [==============================] - 84s 208us/step - loss: 0.0706 - acc: 0.9720 - val_loss: 0.0929 - val_acc: 0.9661\n",
      "Epoch 13/100\n",
      "403797/403797 [==============================] - 84s 207us/step - loss: 0.0718 - acc: 0.9718 - val_loss: 0.0881 - val_acc: 0.9677\n",
      "Epoch 14/100\n",
      "403797/403797 [==============================] - 83s 204us/step - loss: 0.0707 - acc: 0.9724 - val_loss: 0.0800 - val_acc: 0.9698\n",
      "Epoch 15/100\n",
      "403797/403797 [==============================] - 82s 203us/step - loss: 0.0707 - acc: 0.9726 - val_loss: 0.1020 - val_acc: 0.9634\n",
      "Epoch 16/100\n",
      "403797/403797 [==============================] - 81s 202us/step - loss: 0.0716 - acc: 0.9720 - val_loss: 0.1201 - val_acc: 0.9585\n",
      "Epoch 17/100\n",
      "403797/403797 [==============================] - 80s 198us/step - loss: 0.0705 - acc: 0.9723 - val_loss: 0.1041 - val_acc: 0.9626\n",
      "Epoch 18/100\n",
      "403797/403797 [==============================] - 80s 197us/step - loss: 0.0707 - acc: 0.9723 - val_loss: 0.1010 - val_acc: 0.9637\n",
      "Epoch 19/100\n",
      "403797/403797 [==============================] - 80s 198us/step - loss: 0.0708 - acc: 0.9723 - val_loss: 0.0955 - val_acc: 0.9659\n",
      "Epoch 20/100\n",
      "403797/403797 [==============================] - 80s 198us/step - loss: 0.0687 - acc: 0.9731 - val_loss: 0.0955 - val_acc: 0.9664\n",
      "Epoch 21/100\n",
      "403797/403797 [==============================] - 82s 202us/step - loss: 0.0712 - acc: 0.9722 - val_loss: 0.0945 - val_acc: 0.9646\n",
      "Epoch 22/100\n",
      "403797/403797 [==============================] - 82s 203us/step - loss: 0.0705 - acc: 0.9724 - val_loss: 0.0864 - val_acc: 0.9661\n",
      "Epoch 23/100\n",
      "403797/403797 [==============================] - 80s 199us/step - loss: 0.0676 - acc: 0.9734 - val_loss: 0.1108 - val_acc: 0.9596\n",
      "Epoch 24/100\n",
      "403797/403797 [==============================] - 81s 200us/step - loss: 0.0706 - acc: 0.9722 - val_loss: 0.1012 - val_acc: 0.9618\n",
      "Epoch 25/100\n",
      "403797/403797 [==============================] - 80s 197us/step - loss: 0.0705 - acc: 0.9726 - val_loss: 0.0945 - val_acc: 0.9676\n",
      "Epoch 26/100\n",
      "403797/403797 [==============================] - 78s 193us/step - loss: 0.0694 - acc: 0.9727 - val_loss: 0.0875 - val_acc: 0.9695\n",
      "Epoch 27/100\n",
      "403797/403797 [==============================] - 80s 197us/step - loss: 0.0707 - acc: 0.9722 - val_loss: 0.0955 - val_acc: 0.9661\n",
      "Epoch 28/100\n",
      "403797/403797 [==============================] - 79s 194us/step - loss: 0.0693 - acc: 0.9726 - val_loss: 0.0921 - val_acc: 0.9659\n",
      "Epoch 29/100\n",
      "403797/403797 [==============================] - 78s 192us/step - loss: 0.0706 - acc: 0.9722 - val_loss: 0.1013 - val_acc: 0.9623\n",
      "Epoch 30/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0689 - acc: 0.9730 - val_loss: 0.0973 - val_acc: 0.9618\n",
      "Epoch 31/100\n",
      "403797/403797 [==============================] - 78s 194us/step - loss: 0.0688 - acc: 0.9731 - val_loss: 0.1011 - val_acc: 0.9620\n",
      "Epoch 32/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0690 - acc: 0.9728 - val_loss: 0.1034 - val_acc: 0.9635\n",
      "Epoch 33/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0706 - acc: 0.9721 - val_loss: 0.1072 - val_acc: 0.9617\n",
      "Epoch 34/100\n",
      "403797/403797 [==============================] - 77s 190us/step - loss: 0.0691 - acc: 0.9730 - val_loss: 0.1030 - val_acc: 0.9630\n",
      "Epoch 35/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0699 - acc: 0.9726 - val_loss: 0.1196 - val_acc: 0.9575\n",
      "Epoch 36/100\n",
      "403797/403797 [==============================] - 77s 190us/step - loss: 0.0691 - acc: 0.9730 - val_loss: 0.1009 - val_acc: 0.9655\n",
      "Epoch 37/100\n",
      "403797/403797 [==============================] - 77s 190us/step - loss: 0.0694 - acc: 0.9728 - val_loss: 0.1367 - val_acc: 0.9545\n",
      "Epoch 38/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0700 - acc: 0.9728 - val_loss: 0.0904 - val_acc: 0.9677\n",
      "Epoch 39/100\n",
      "403797/403797 [==============================] - 77s 190us/step - loss: 0.0681 - acc: 0.9734 - val_loss: 0.0949 - val_acc: 0.9642\n",
      "Epoch 40/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0697 - acc: 0.9727 - val_loss: 0.0856 - val_acc: 0.9687\n",
      "Epoch 41/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0698 - acc: 0.9725 - val_loss: 0.1006 - val_acc: 0.9637\n",
      "Epoch 42/100\n",
      "403797/403797 [==============================] - 77s 192us/step - loss: 0.0685 - acc: 0.9732 - val_loss: 0.1131 - val_acc: 0.9596\n",
      "Epoch 43/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0690 - acc: 0.9730 - val_loss: 0.0869 - val_acc: 0.9666\n",
      "Epoch 44/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0713 - acc: 0.9721 - val_loss: 0.0927 - val_acc: 0.9644\n",
      "Epoch 45/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0679 - acc: 0.9732 - val_loss: 0.0965 - val_acc: 0.9665\n",
      "Epoch 46/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0688 - acc: 0.9729 - val_loss: 0.1014 - val_acc: 0.9631\n",
      "Epoch 47/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0701 - acc: 0.9726 - val_loss: 0.0864 - val_acc: 0.9675\n",
      "Epoch 48/100\n",
      "403797/403797 [==============================] - 77s 191us/step - loss: 0.0684 - acc: 0.9731 - val_loss: 0.0984 - val_acc: 0.9665\n",
      "Epoch 49/100\n",
      "403797/403797 [==============================] - 80s 197us/step - loss: 0.0647 - acc: 0.9747 - val_loss: 0.0943 - val_acc: 0.9653\n",
      "Epoch 96/100\n",
      "403797/403797 [==============================] - 78s 193us/step - loss: 0.0674 - acc: 0.9737 - val_loss: 0.0782 - val_acc: 0.9720\n",
      "Epoch 97/100\n",
      "403797/403797 [==============================] - 77s 190us/step - loss: 0.0655 - acc: 0.9744 - val_loss: 0.0987 - val_acc: 0.9644\n",
      "Epoch 98/100\n",
      "215552/403797 [===============>..............] - ETA: 35s - loss: 0.0646 - acc: 0.9745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "\n",
    "model.save(r\"floyd_model_xxl_data_ver14.h5\")\n",
    "print('Training is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "keras-wide-deep.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
