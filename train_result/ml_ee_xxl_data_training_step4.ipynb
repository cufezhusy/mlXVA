{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 321,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2880,
     "status": "error",
     "timestamp": 1505781339378,
     "user": {
      "displayName": "Sara Robinson",
      "photoUrl": "//lh4.googleusercontent.com/-RR9n0dvbwgI/AAAAAAAAAAI/AAAAAAAAMYM/SOr5ZExpvXE/s50-c-k-no/photo.jpg",
      "userId": "112510032804989247452"
     },
     "user_tz": 240
    },
    "id": "783h64rGhA3T",
    "outputId": "d447b2ab-e321-4ee5-abd4-de2c0116302f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\n",
    "\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- We are using CPU now ----\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print('---- We are using GPU now ----')\n",
    "    # GPU\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 80  # Number of passes through entire dataset\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    print('---- We are using CPU now ----')\n",
    "    # CPU\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The wine reviews dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X :(412038, 240, 1)\n",
      "Size of Y :(412038,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/floyd/input/gengduoshuju/'  # ADD path/to/dataset\n",
    "Y= pickle.load( open(os.path.join(data_path,'Y.pks'), \"rb\" ) )\n",
    "X= pickle.load( open(os.path.join(data_path,'X.pks'), \"rb\" ) )\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "print(\"Size of X :\" + str(X.shape))\n",
    "print(\"Size of Y :\" + str(Y.shape))\n",
    "X = X.astype(np.float64)\n",
    "X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "331.0\n",
      "number of training examples = 403797\n",
      "number of test examples = 8241\n",
      "X_train shape: (403797, 240, 1)\n",
      "Y_train shape: (403797, 332)\n",
      "X_test shape: (8241, 240, 1)\n",
      "Y_test shape: (8241, 332)\n"
     ]
    }
   ],
   "source": [
    "X_train,  X_test, Y_train_orig,Y_test_orig= helper.divide_data(X,Y)\n",
    "print(Y.min())\n",
    "print(Y.max())\n",
    "num_classes = 332\n",
    "Y_train = keras.utils.to_categorical(Y_train_orig, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test_orig, num_classes)\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# Load the model what has already ben trained\n",
    "# ===================================================================================\n",
    "\n",
    "model = load_model(r\"floyd_model_xxl_data_ver5.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 240, 16)           80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 240, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 60, 64)            8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 64)            16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 15, 32)            8224      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 332)               85324     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 332)               0         \n",
      "=================================================================\n",
      "Total params: 120,412\n",
      "Trainable params: 120,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 403797 samples, validate on 8241 samples\n",
      "Epoch 1/100\n",
      "403797/403797 [==============================] - 221s 548us/step - loss: 0.3275 - acc: 0.8866 - val_loss: 0.3831 - val_acc: 0.8697\n",
      "Epoch 2/100\n",
      "403797/403797 [==============================] - 221s 546us/step - loss: 0.3261 - acc: 0.8863 - val_loss: 0.3265 - val_acc: 0.8834\n",
      "Epoch 3/100\n",
      "403797/403797 [==============================] - 221s 548us/step - loss: 0.3156 - acc: 0.8895 - val_loss: 0.3059 - val_acc: 0.8938\n",
      "Epoch 4/100\n",
      "403797/403797 [==============================] - 222s 549us/step - loss: 0.3259 - acc: 0.8868 - val_loss: 0.3218 - val_acc: 0.8881\n",
      "Epoch 5/100\n",
      "403797/403797 [==============================] - 222s 550us/step - loss: 0.3128 - acc: 0.8909 - val_loss: 0.3497 - val_acc: 0.8777\n",
      "Epoch 6/100\n",
      "403797/403797 [==============================] - 222s 550us/step - loss: 0.3140 - acc: 0.8907 - val_loss: 0.3463 - val_acc: 0.8821\n",
      "Epoch 7/100\n",
      "403797/403797 [==============================] - 222s 549us/step - loss: 0.3149 - acc: 0.8897 - val_loss: 0.3487 - val_acc: 0.8814\n",
      "Epoch 8/100\n",
      "403797/403797 [==============================] - 221s 548us/step - loss: 0.3159 - acc: 0.8896 - val_loss: 0.3307 - val_acc: 0.8874\n",
      "Epoch 9/100\n",
      "403797/403797 [==============================] - 223s 551us/step - loss: 0.3139 - acc: 0.8901 - val_loss: 0.3348 - val_acc: 0.8825\n",
      "Epoch 10/100\n",
      "403797/403797 [==============================] - 223s 552us/step - loss: 0.3085 - acc: 0.8920 - val_loss: 0.3502 - val_acc: 0.8725\n",
      "Epoch 11/100\n",
      "403797/403797 [==============================] - 224s 554us/step - loss: 0.3152 - acc: 0.8907 - val_loss: 0.3257 - val_acc: 0.8924\n",
      "Epoch 12/100\n",
      "403797/403797 [==============================] - 222s 550us/step - loss: 0.3089 - acc: 0.8917 - val_loss: 0.3910 - val_acc: 0.8649\n",
      "Epoch 13/100\n",
      "403797/403797 [==============================] - 222s 550us/step - loss: 0.3017 - acc: 0.8943 - val_loss: 0.3114 - val_acc: 0.8907\n",
      "Epoch 14/100\n",
      "403797/403797 [==============================] - 221s 548us/step - loss: 0.3191 - acc: 0.8911 - val_loss: 0.5409 - val_acc: 0.8231\n",
      "Epoch 15/100\n",
      "403797/403797 [==============================] - 223s 552us/step - loss: 0.2890 - acc: 0.8989 - val_loss: 0.2832 - val_acc: 0.9034\n",
      "Epoch 16/100\n",
      "403797/403797 [==============================] - 223s 551us/step - loss: 0.3009 - acc: 0.8947 - val_loss: 0.2993 - val_acc: 0.8990\n",
      "Epoch 17/100\n",
      "403797/403797 [==============================] - 222s 550us/step - loss: 0.2976 - acc: 0.8957 - val_loss: 0.3316 - val_acc: 0.8867\n",
      "Epoch 18/100\n",
      "403797/403797 [==============================] - 223s 553us/step - loss: 0.2986 - acc: 0.8952 - val_loss: 0.2933 - val_acc: 0.9003\n",
      "Epoch 19/100\n",
      "403797/403797 [==============================] - 222s 550us/step - loss: 0.2993 - acc: 0.8949 - val_loss: 0.3157 - val_acc: 0.8936\n",
      "Epoch 20/100\n",
      "403797/403797 [==============================] - 222s 549us/step - loss: 0.2941 - acc: 0.8970 - val_loss: 0.2969 - val_acc: 0.8915\n",
      "Epoch 21/100\n",
      "403797/403797 [==============================] - 222s 551us/step - loss: 0.2911 - acc: 0.8975 - val_loss: 0.3576 - val_acc: 0.8710\n",
      "Epoch 22/100\n",
      "403797/403797 [==============================] - 222s 549us/step - loss: 0.2883 - acc: 0.8984 - val_loss: 0.3032 - val_acc: 0.8954\n",
      "Epoch 23/100\n",
      "403797/403797 [==============================] - 222s 549us/step - loss: 0.2898 - acc: 0.8988 - val_loss: 0.2908 - val_acc: 0.8971\n",
      "Epoch 24/100\n",
      "403797/403797 [==============================] - 223s 552us/step - loss: 0.2816 - acc: 0.9003 - val_loss: 0.2783 - val_acc: 0.9038\n",
      "Epoch 29/100\n",
      "403797/403797 [==============================] - 222s 551us/step - loss: 0.2773 - acc: 0.9018 - val_loss: 0.2824 - val_acc: 0.8993\n",
      "Epoch 30/100\n",
      "403797/403797 [==============================] - 222s 549us/step - loss: 0.2769 - acc: 0.9025 - val_loss: 0.3116 - val_acc: 0.8910\n",
      "Epoch 31/100\n",
      "403797/403797 [==============================] - 220s 546us/step - loss: 0.2797 - acc: 0.9021 - val_loss: 0.2985 - val_acc: 0.8926\n",
      "Epoch 32/100\n",
      "403797/403797 [==============================] - 221s 547us/step - loss: 0.2748 - acc: 0.9033 - val_loss: 0.2981 - val_acc: 0.8959\n",
      "Epoch 33/100\n",
      "178944/403797 [============>.................] - ETA: 2:07 - loss: 0.2866 - acc: 0.8994"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "\n",
    "model.save(r\"floyd_model_xxl_data_ver6.h5\")\n",
    "print('Training is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "keras-wide-deep.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
