{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "height": 321.0,
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2880.0,
     "status": "error",
     "timestamp": 1.505781339378E12,
     "user": {
      "displayName": "Sara Robinson",
      "photoUrl": "//lh4.googleusercontent.com/-RR9n0dvbwgI/AAAAAAAAAAI/AAAAAAAAMYM/SOr5ZExpvXE/s50-c-k-no/photo.jpg",
      "userId": "112510032804989247452"
     },
     "user_tz": 240.0
    },
    "id": "783h64rGhA3T",
    "outputId": "d447b2ab-e321-4ee5-abd4-de2c0116302f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\n",
    "\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- We are using GPU now ----\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print('---- We are using GPU now ----')\n",
    "    # GPU\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 80  # Number of passes through entire dataset\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    print('---- We are using CPU now ----')\n",
    "    # CPU\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The wine reviews dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X :(412038, 240, 1)\n",
      "Size of Y :(412038,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/floyd/input/gengduoshuju/'  # ADD path/to/dataset\n",
    "Y= pickle.load( open(os.path.join(data_path,'Y.pks'), \"rb\" ) )\n",
    "X= pickle.load( open(os.path.join(data_path,'X.pks'), \"rb\" ) )\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "print(\"Size of X :\" + str(X.shape))\n",
    "print(\"Size of Y :\" + str(Y.shape))\n",
    "X = X.astype(np.float64)\n",
    "X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "331.0\n",
      "number of training examples = 403797\n",
      "number of test examples = 8241\n",
      "X_train shape: (403797, 240, 1)\n",
      "Y_train shape: (403797, 332)\n",
      "X_test shape: (8241, 240, 1)\n",
      "Y_test shape: (8241, 332)\n"
     ]
    }
   ],
   "source": [
    "X_train,  X_test, Y_train_orig,Y_test_orig= helper.divide_data(X,Y)\n",
    "print(Y.min())\n",
    "print(Y.max())\n",
    "num_classes = 332\n",
    "Y_train = keras.utils.to_categorical(Y_train_orig, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test_orig, num_classes)\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# Load the model what has already ben trained\n",
    "# ===================================================================================\n",
    "\n",
    "model = load_model(r\"floyd_model_xxl_data_ver4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 240, 16)           80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 240, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 60, 64)            8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 64)            16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 15, 32)            8224      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 332)               85324     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 332)               0         \n",
      "=================================================================\n",
      "Total params: 120,412\n",
      "Trainable params: 120,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 403797 samples, validate on 8241 samples\n",
      "Epoch 1/80\n",
      "403797/403797 [==============================] - 19s 47us/step - loss: 0.3704 - acc: 0.8732 - val_loss: 0.4076 - val_acc: 0.8546\n",
      "Epoch 2/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3713 - acc: 0.8708 - val_loss: 0.3859 - val_acc: 0.8623\n",
      "Epoch 3/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3864 - acc: 0.8695 - val_loss: 0.3751 - val_acc: 0.8708\n",
      "Epoch 4/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3580 - acc: 0.8760 - val_loss: 0.3858 - val_acc: 0.8674\n",
      "Epoch 5/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3674 - acc: 0.8727 - val_loss: 0.3655 - val_acc: 0.8698\n",
      "Epoch 6/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3872 - acc: 0.8689 - val_loss: 0.4578 - val_acc: 0.8390\n",
      "Epoch 7/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3490 - acc: 0.8794 - val_loss: 0.3636 - val_acc: 0.8717\n",
      "Epoch 8/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3552 - acc: 0.8771 - val_loss: 0.3687 - val_acc: 0.8762\n",
      "Epoch 9/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3537 - acc: 0.8777 - val_loss: 0.3743 - val_acc: 0.8657\n",
      "Epoch 10/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3557 - acc: 0.8763 - val_loss: 0.3840 - val_acc: 0.8692\n",
      "Epoch 11/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3648 - acc: 0.8763 - val_loss: 0.3369 - val_acc: 0.8869\n",
      "Epoch 12/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3470 - acc: 0.8793 - val_loss: 0.3553 - val_acc: 0.8768\n",
      "Epoch 13/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3503 - acc: 0.8788 - val_loss: 0.3490 - val_acc: 0.8763\n",
      "Epoch 14/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3474 - acc: 0.8792 - val_loss: 0.3448 - val_acc: 0.8776\n",
      "Epoch 15/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3481 - acc: 0.8790 - val_loss: 0.3707 - val_acc: 0.8658\n",
      "Epoch 16/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3398 - acc: 0.8818 - val_loss: 0.3641 - val_acc: 0.8750\n",
      "Epoch 17/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3459 - acc: 0.8791 - val_loss: 0.3686 - val_acc: 0.8703\n",
      "Epoch 18/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3423 - acc: 0.8807 - val_loss: 0.3632 - val_acc: 0.8762\n",
      "Epoch 19/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3601 - acc: 0.8772 - val_loss: 0.3577 - val_acc: 0.8776\n",
      "Epoch 20/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3260 - acc: 0.8870 - val_loss: 0.4318 - val_acc: 0.8475\n",
      "Epoch 21/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3442 - acc: 0.8810 - val_loss: 0.3380 - val_acc: 0.8828\n",
      "Epoch 22/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3264 - acc: 0.8864 - val_loss: 0.3412 - val_acc: 0.8819\n",
      "Epoch 23/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3308 - acc: 0.8851 - val_loss: 0.3483 - val_acc: 0.8771\n",
      "Epoch 24/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3433 - acc: 0.8817 - val_loss: 0.3443 - val_acc: 0.8831\n",
      "Epoch 25/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3217 - acc: 0.8883 - val_loss: 0.3504 - val_acc: 0.8753\n",
      "Epoch 26/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3719 - acc: 0.8778 - val_loss: 0.3260 - val_acc: 0.8830\n",
      "Epoch 27/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3113 - acc: 0.8921 - val_loss: 0.3354 - val_acc: 0.8842\n",
      "Epoch 28/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3227 - acc: 0.8870 - val_loss: 0.3323 - val_acc: 0.8857\n",
      "Epoch 29/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3206 - acc: 0.8882 - val_loss: 0.3033 - val_acc: 0.8933\n",
      "Epoch 30/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3256 - acc: 0.8865 - val_loss: 0.3244 - val_acc: 0.8857\n",
      "Epoch 31/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3179 - acc: 0.8888 - val_loss: 0.3122 - val_acc: 0.8920\n",
      "Epoch 32/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3263 - acc: 0.8873 - val_loss: 0.4216 - val_acc: 0.8527\n",
      "Epoch 33/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3099 - acc: 0.8924 - val_loss: 0.3196 - val_acc: 0.8887\n",
      "Epoch 34/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3145 - acc: 0.8902 - val_loss: 0.3444 - val_acc: 0.8821\n",
      "Epoch 35/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3550 - acc: 0.8823 - val_loss: 0.3659 - val_acc: 0.8705\n",
      "Epoch 36/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.2964 - acc: 0.8972 - val_loss: 0.3094 - val_acc: 0.8942\n",
      "Epoch 37/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3035 - acc: 0.8940 - val_loss: 0.2934 - val_acc: 0.8971\n",
      "Epoch 38/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3074 - acc: 0.8921 - val_loss: 0.3288 - val_acc: 0.8822\n",
      "Epoch 39/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3151 - acc: 0.8897 - val_loss: 0.3246 - val_acc: 0.8828\n",
      "Epoch 40/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3152 - acc: 0.8906 - val_loss: 0.3010 - val_acc: 0.8904\n",
      "Epoch 41/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.2984 - acc: 0.8960 - val_loss: 0.3287 - val_acc: 0.8838\n",
      "Epoch 42/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3060 - acc: 0.8939 - val_loss: 1.0902 - val_acc: 0.7146\n",
      "Epoch 43/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3016 - acc: 0.8953 - val_loss: 0.3168 - val_acc: 0.8920\n",
      "Epoch 44/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.2985 - acc: 0.8958 - val_loss: 0.3180 - val_acc: 0.8888\n",
      "Epoch 45/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3026 - acc: 0.8935 - val_loss: 0.3316 - val_acc: 0.8859\n",
      "Epoch 46/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2990 - acc: 0.8957 - val_loss: 0.3262 - val_acc: 0.8895\n",
      "Epoch 47/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2967 - acc: 0.8959 - val_loss: 0.3361 - val_acc: 0.8790\n",
      "Epoch 48/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2989 - acc: 0.8953 - val_loss: 0.3336 - val_acc: 0.8794\n",
      "Epoch 49/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2931 - acc: 0.8976 - val_loss: 0.3155 - val_acc: 0.8920\n",
      "Epoch 50/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.3200 - acc: 0.8934 - val_loss: 0.2988 - val_acc: 0.8970\n",
      "Epoch 51/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2822 - acc: 0.9015 - val_loss: 0.3211 - val_acc: 0.8890\n",
      "Epoch 52/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2909 - acc: 0.8976 - val_loss: 0.3113 - val_acc: 0.8912\n",
      "Epoch 53/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2890 - acc: 0.8990 - val_loss: 0.3115 - val_acc: 0.8942\n",
      "Epoch 54/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.3050 - acc: 0.8967 - val_loss: 0.2917 - val_acc: 0.8963\n",
      "Epoch 55/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2810 - acc: 0.9021 - val_loss: 0.3534 - val_acc: 0.8759\n",
      "Epoch 56/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2902 - acc: 0.8982 - val_loss: 0.2863 - val_acc: 0.9021\n",
      "Epoch 57/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2874 - acc: 0.8995 - val_loss: 0.3143 - val_acc: 0.8863\n",
      "Epoch 58/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3159 - acc: 0.8951 - val_loss: 0.2838 - val_acc: 0.9003\n",
      "Epoch 59/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2726 - acc: 0.9048 - val_loss: 0.3122 - val_acc: 0.8896\n",
      "Epoch 60/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.3136 - acc: 0.8963 - val_loss: 0.2896 - val_acc: 0.9023\n",
      "Epoch 61/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2688 - acc: 0.9065 - val_loss: 0.3178 - val_acc: 0.8840\n",
      "Epoch 62/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2776 - acc: 0.9025 - val_loss: 0.2916 - val_acc: 0.8963\n",
      "Epoch 63/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2783 - acc: 0.9025 - val_loss: 0.2858 - val_acc: 0.8971\n",
      "Epoch 64/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2841 - acc: 0.9010 - val_loss: 0.2776 - val_acc: 0.9033\n",
      "Epoch 65/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2754 - acc: 0.9032 - val_loss: 0.2798 - val_acc: 0.9017\n",
      "Epoch 66/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2715 - acc: 0.9047 - val_loss: 0.2959 - val_acc: 0.8960\n",
      "Epoch 67/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2973 - acc: 0.8983 - val_loss: 0.3085 - val_acc: 0.8922\n",
      "Epoch 68/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2649 - acc: 0.9070 - val_loss: 0.3012 - val_acc: 0.8978\n",
      "Epoch 69/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2702 - acc: 0.9048 - val_loss: 0.2855 - val_acc: 0.8973\n",
      "Epoch 70/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3104 - acc: 0.8982 - val_loss: 0.2889 - val_acc: 0.8992\n",
      "Epoch 71/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2585 - acc: 0.9096 - val_loss: 0.3127 - val_acc: 0.8926\n",
      "Epoch 72/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2656 - acc: 0.9068 - val_loss: 0.2848 - val_acc: 0.8982\n",
      "Epoch 73/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2706 - acc: 0.9050 - val_loss: 0.2786 - val_acc: 0.9011\n",
      "Epoch 74/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2955 - acc: 0.9005 - val_loss: 0.2653 - val_acc: 0.9104\n",
      "Epoch 75/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2532 - acc: 0.9109 - val_loss: 0.2842 - val_acc: 0.9007\n",
      "Epoch 76/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2647 - acc: 0.9070 - val_loss: 0.3061 - val_acc: 0.8932\n",
      "Epoch 77/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.2643 - acc: 0.9065 - val_loss: 0.2799 - val_acc: 0.9017\n",
      "Epoch 78/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2650 - acc: 0.9066 - val_loss: 0.3939 - val_acc: 0.8618\n",
      "Epoch 79/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2970 - acc: 0.9028 - val_loss: 0.2735 - val_acc: 0.9057\n",
      "Epoch 80/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.2521 - acc: 0.9112 - val_loss: 0.3087 - val_acc: 0.8891\n",
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "\n",
    "model.save(r\"floyd_model_xxl_data_ver5.h5\")\n",
    "print('Training is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "keras-wide-deep.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
