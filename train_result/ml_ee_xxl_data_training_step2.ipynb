{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 321,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2880,
     "status": "error",
     "timestamp": 1505781339378,
     "user": {
      "displayName": "Sara Robinson",
      "photoUrl": "//lh4.googleusercontent.com/-RR9n0dvbwgI/AAAAAAAAAAI/AAAAAAAAMYM/SOr5ZExpvXE/s50-c-k-no/photo.jpg",
      "userId": "112510032804989247452"
     },
     "user_tz": 240
    },
    "id": "783h64rGhA3T",
    "outputId": "d447b2ab-e321-4ee5-abd4-de2c0116302f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\n",
    "\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- We are using GPU now ----\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print('---- We are using GPU now ----')\n",
    "    # GPU\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 80  # Number of passes through entire dataset\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    print('---- We are using CPU now ----')\n",
    "    # CPU\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The wine reviews dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X :(412038, 240, 1)\n",
      "Size of Y :(412038,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/floyd/input/gengduoshuju/'  # ADD path/to/dataset\n",
    "Y= pickle.load( open(os.path.join(data_path,'Y.pks'), \"rb\" ) )\n",
    "X= pickle.load( open(os.path.join(data_path,'X.pks'), \"rb\" ) )\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "print(\"Size of X :\" + str(X.shape))\n",
    "print(\"Size of Y :\" + str(Y.shape))\n",
    "X = X.astype(np.float64)\n",
    "X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "331.0\n",
      "number of training examples = 403797\n",
      "number of test examples = 8241\n",
      "X_train shape: (403797, 240, 1)\n",
      "Y_train shape: (403797, 332)\n",
      "X_test shape: (8241, 240, 1)\n",
      "Y_test shape: (8241, 332)\n"
     ]
    }
   ],
   "source": [
    "X_train,  X_test, Y_train_orig,Y_test_orig= helper.divide_data(X,Y)\n",
    "print(Y.min())\n",
    "print(Y.max())\n",
    "num_classes = 332\n",
    "Y_train = keras.utils.to_categorical(Y_train_orig, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test_orig, num_classes)\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# Load the model what has already ben trained\n",
    "# ===================================================================================\n",
    "\n",
    "model = load_model(r\"floyd_model_xxl_data_ver3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 240, 16)           80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 240, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 60, 64)            8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 64)            16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 15, 32)            8224      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 332)               85324     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 332)               0         \n",
      "=================================================================\n",
      "Total params: 120,412\n",
      "Trainable params: 120,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 403797 samples, validate on 8241 samples\n",
      "Epoch 1/80\n",
      "403797/403797 [==============================] - 20s 48us/step - loss: 0.6885 - acc: 0.7685 - val_loss: 0.6893 - val_acc: 0.7692\n",
      "Epoch 2/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6821 - acc: 0.7685 - val_loss: 0.7136 - val_acc: 0.7563\n",
      "Epoch 3/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6727 - acc: 0.7716 - val_loss: 0.6883 - val_acc: 0.7677\n",
      "Epoch 4/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6691 - acc: 0.7738 - val_loss: 0.6668 - val_acc: 0.7826\n",
      "Epoch 5/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6630 - acc: 0.7753 - val_loss: 0.6487 - val_acc: 0.7817\n",
      "Epoch 6/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6589 - acc: 0.7767 - val_loss: 0.6805 - val_acc: 0.7664\n",
      "Epoch 7/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6427 - acc: 0.7816 - val_loss: 0.6820 - val_acc: 0.7708\n",
      "Epoch 8/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6430 - acc: 0.7819 - val_loss: 0.6516 - val_acc: 0.7818\n",
      "Epoch 9/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6327 - acc: 0.7848 - val_loss: 0.6458 - val_acc: 0.7822\n",
      "Epoch 10/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6283 - acc: 0.7858 - val_loss: 0.6532 - val_acc: 0.7771\n",
      "Epoch 11/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6223 - acc: 0.7883 - val_loss: 0.6542 - val_acc: 0.7798\n",
      "Epoch 12/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6217 - acc: 0.7883 - val_loss: 0.6376 - val_acc: 0.7799\n",
      "Epoch 13/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.6098 - acc: 0.7927 - val_loss: 0.6238 - val_acc: 0.7859\n",
      "Epoch 14/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.6025 - acc: 0.7945 - val_loss: 0.6262 - val_acc: 0.7806\n",
      "Epoch 15/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5981 - acc: 0.7965 - val_loss: 0.6339 - val_acc: 0.7834\n",
      "Epoch 16/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5931 - acc: 0.7974 - val_loss: 0.6239 - val_acc: 0.7872\n",
      "Epoch 17/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5958 - acc: 0.7968 - val_loss: 0.6342 - val_acc: 0.7833\n",
      "Epoch 18/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5809 - acc: 0.8013 - val_loss: 0.6226 - val_acc: 0.7965\n",
      "Epoch 19/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5750 - acc: 0.8034 - val_loss: 0.5778 - val_acc: 0.8031\n",
      "Epoch 20/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5703 - acc: 0.8053 - val_loss: 0.6099 - val_acc: 0.7876\n",
      "Epoch 21/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.5737 - acc: 0.8036 - val_loss: 0.5647 - val_acc: 0.8080\n",
      "Epoch 22/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5601 - acc: 0.8080 - val_loss: 0.6101 - val_acc: 0.7929\n",
      "Epoch 23/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5555 - acc: 0.8100 - val_loss: 0.5879 - val_acc: 0.8010\n",
      "Epoch 24/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5586 - acc: 0.8086 - val_loss: 0.5775 - val_acc: 0.8049\n",
      "Epoch 25/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5495 - acc: 0.8122 - val_loss: 0.5661 - val_acc: 0.8055\n",
      "Epoch 26/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5458 - acc: 0.8126 - val_loss: 0.6930 - val_acc: 0.7639\n",
      "Epoch 27/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5368 - acc: 0.8162 - val_loss: 0.5663 - val_acc: 0.8046\n",
      "Epoch 28/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5369 - acc: 0.8163 - val_loss: 0.5626 - val_acc: 0.8107\n",
      "Epoch 29/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5307 - acc: 0.8185 - val_loss: 0.5223 - val_acc: 0.8249\n",
      "Epoch 30/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5301 - acc: 0.8183 - val_loss: 0.5294 - val_acc: 0.8196\n",
      "Epoch 31/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5184 - acc: 0.8221 - val_loss: 0.5147 - val_acc: 0.8232\n",
      "Epoch 32/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5163 - acc: 0.8232 - val_loss: 0.5819 - val_acc: 0.7971\n",
      "Epoch 33/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5113 - acc: 0.8245 - val_loss: 0.5640 - val_acc: 0.8039\n",
      "Epoch 34/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.5122 - acc: 0.8243 - val_loss: 0.5823 - val_acc: 0.7998\n",
      "Epoch 35/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.5032 - acc: 0.8271 - val_loss: 0.5636 - val_acc: 0.8142\n",
      "Epoch 36/80\n",
      "403797/403797 [==============================] - 17s 42us/step - loss: 0.5051 - acc: 0.8263 - val_loss: 0.5021 - val_acc: 0.8316\n",
      "Epoch 37/80\n",
      "403797/403797 [==============================] - 17s 42us/step - loss: 0.4944 - acc: 0.8294 - val_loss: 0.4801 - val_acc: 0.8372\n",
      "Epoch 38/80\n",
      "403797/403797 [==============================] - 17s 42us/step - loss: 0.4951 - acc: 0.8303 - val_loss: 0.5234 - val_acc: 0.8169\n",
      "Epoch 39/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.4942 - acc: 0.8303 - val_loss: 0.4914 - val_acc: 0.8289\n",
      "Epoch 40/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.4813 - acc: 0.8340 - val_loss: 0.4762 - val_acc: 0.8370\n",
      "Epoch 41/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.4835 - acc: 0.8343 - val_loss: 0.4968 - val_acc: 0.8340\n",
      "Epoch 42/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.4751 - acc: 0.8365 - val_loss: 0.5188 - val_acc: 0.8221\n",
      "Epoch 43/80\n",
      "403797/403797 [==============================] - 17s 41us/step - loss: 0.4943 - acc: 0.8319 - val_loss: 0.4778 - val_acc: 0.8384\n",
      "Epoch 44/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4633 - acc: 0.8405 - val_loss: 0.4776 - val_acc: 0.8363\n",
      "Epoch 45/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4693 - acc: 0.8382 - val_loss: 0.5133 - val_acc: 0.8207\n",
      "Epoch 46/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4637 - acc: 0.8404 - val_loss: 0.4806 - val_acc: 0.8359\n",
      "Epoch 47/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4642 - acc: 0.8402 - val_loss: 0.4685 - val_acc: 0.8375\n",
      "Epoch 48/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4572 - acc: 0.8423 - val_loss: 0.4771 - val_acc: 0.8341\n",
      "Epoch 49/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4689 - acc: 0.8413 - val_loss: 0.4542 - val_acc: 0.8492\n",
      "Epoch 50/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4436 - acc: 0.8476 - val_loss: 0.4551 - val_acc: 0.8429\n",
      "Epoch 51/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4500 - acc: 0.8455 - val_loss: 0.4434 - val_acc: 0.8495\n",
      "Epoch 52/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4431 - acc: 0.8472 - val_loss: 0.4472 - val_acc: 0.8503\n",
      "Epoch 53/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4530 - acc: 0.8444 - val_loss: 0.4565 - val_acc: 0.8472\n",
      "Epoch 54/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4343 - acc: 0.8503 - val_loss: 0.4607 - val_acc: 0.8424\n",
      "Epoch 55/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4456 - acc: 0.8467 - val_loss: 0.4703 - val_acc: 0.8402\n",
      "Epoch 56/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4305 - acc: 0.8513 - val_loss: 0.4339 - val_acc: 0.8480\n",
      "Epoch 57/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4300 - acc: 0.8519 - val_loss: 0.5286 - val_acc: 0.8140\n",
      "Epoch 58/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4256 - acc: 0.8532 - val_loss: 0.4378 - val_acc: 0.8484\n",
      "Epoch 59/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4277 - acc: 0.8522 - val_loss: 0.4381 - val_acc: 0.8497\n",
      "Epoch 60/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4244 - acc: 0.8531 - val_loss: 0.4503 - val_acc: 0.8473\n",
      "Epoch 61/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4197 - acc: 0.8548 - val_loss: 0.5181 - val_acc: 0.8185\n",
      "Epoch 62/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4180 - acc: 0.8554 - val_loss: 0.4394 - val_acc: 0.8467\n",
      "Epoch 63/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4204 - acc: 0.8554 - val_loss: 0.4110 - val_acc: 0.8561\n",
      "Epoch 64/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4077 - acc: 0.8593 - val_loss: 0.4276 - val_acc: 0.8506\n",
      "Epoch 65/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4125 - acc: 0.8575 - val_loss: 0.3923 - val_acc: 0.8671\n",
      "Epoch 66/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4033 - acc: 0.8603 - val_loss: 0.4533 - val_acc: 0.8397\n",
      "Epoch 67/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4230 - acc: 0.8546 - val_loss: 0.4453 - val_acc: 0.8449\n",
      "Epoch 68/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3949 - acc: 0.8639 - val_loss: 0.4663 - val_acc: 0.8442\n",
      "Epoch 69/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3996 - acc: 0.8617 - val_loss: 0.3913 - val_acc: 0.8662\n",
      "Epoch 70/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3965 - acc: 0.8633 - val_loss: 0.4298 - val_acc: 0.8518\n",
      "Epoch 71/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.4204 - acc: 0.8582 - val_loss: 0.4522 - val_acc: 0.8446\n",
      "Epoch 72/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3861 - acc: 0.8663 - val_loss: 0.4080 - val_acc: 0.8597\n",
      "Epoch 73/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3924 - acc: 0.8638 - val_loss: 0.3969 - val_acc: 0.8640\n",
      "Epoch 74/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3927 - acc: 0.8647 - val_loss: 0.4086 - val_acc: 0.8614\n",
      "Epoch 75/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3796 - acc: 0.8685 - val_loss: 0.4106 - val_acc: 0.8551\n",
      "Epoch 76/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3915 - acc: 0.8647 - val_loss: 0.3849 - val_acc: 0.8679\n",
      "Epoch 77/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3792 - acc: 0.8683 - val_loss: 0.4030 - val_acc: 0.8558\n",
      "Epoch 78/80\n",
      "403797/403797 [==============================] - 16s 40us/step - loss: 0.3795 - acc: 0.8687 - val_loss: 0.4230 - val_acc: 0.8528\n",
      "Epoch 79/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3968 - acc: 0.8674 - val_loss: 0.3837 - val_acc: 0.8708\n",
      "Epoch 80/80\n",
      "403797/403797 [==============================] - 16s 41us/step - loss: 0.3684 - acc: 0.8725 - val_loss: 0.4293 - val_acc: 0.8477\n",
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "\n",
    "model.save(r\"floyd_model_xxl_data_ver4.h5\")\n",
    "print('Training is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "keras-wide-deep.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
